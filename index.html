<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<style>
  .bottom-three {
      margin-bottom: 5cm;
  }
</style>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <link href="main.css" rel="stylesheet" media="all">
    <meta name="description" content="Text-to-Audio Generation via Bridging Audio Language Model and Latent Diffusion" />
    <meta name="keywords" content="Diffusion models, Text-to-Audio Generation, Generative AI, LLM, AIGC, Transformer, Cross Attention">
    <script>
    function buttonSwitch(id, text) {
        old_src = document.getElementById(id).src;
        ind = old_src.lastIndexOf('/');
        document.getElementById(id).src = old_src.substr(0, ind + 1) + text;
    }
    </script>
    <title>Text-to-Audio Generation via Bridging Audio Language Model and Latent Diffusion</title>
</head>




<body>
    <div id="top_arrow" style="position: fixed; bottom: 10px; right: 10px;">
        <a href="#title"><img src="./figures/top_arrow.jpg" style="border: 0pt none ; width: 26px; height: 26px;"/></a>
    </div>
    
    <!-- <p  style="margin-bottom:3cm;">"  "</p> -->
    <h2 id="title" class="auto-style1"><center>  Text-to-Audio Generation via Bridging Audio Language Model and Latent Diffusion </center></h2>


        
        <!-- Image with description -->
        <p class="auto-style5">&nbsp;</p>
        <p class="style2", style="color:blue;"><strong><span class="auto-style6"><u>Proposed Dual-Conditioned Latent Diffusion Model</u></span></strong></p>
        <table style="width:1250px" align="center">
            <tr><td><center><img width=1250px alt="" src="figures/arc.pdf"></center></td></tr>
            <tr><td><p align="center" class="auto-style5-j">Figure 1. Dual-conditioned Latent Diffusion Model Architecture.</p></td></tr>
        </table>

        <!-- Outline -->
        <p class="auto-style5">&nbsp;</p>
        <p id="performance" , class="auto-style4", style="color:blue;"><strong><u>Abstract: </u></strong></p>
        <p class="auto-style4" align="left"> Diffusion models empower the majority of text-to-audio (TTA) generation approaches. Some recent diffusion-based TTA methods use a large text encoder to encode the textual description of the generated audio, which acts as a semantic condition to guide the audio generation. In this work, we build on top of the commonly-used diffusion model architecture, and integrate another acoustic condition into the diffusion process. We adopt an auto-regressive generative model that generates audio tokens conditioned on text inputs, then audio tokens can be converted into acoustic latent features as the additional condition representations to refine the audio generation outcome. Consequently, our dual-conditioned latent diffusion approach delivers the state-of-the-art results on most metrics and stays comparable on the rest of results on AudioCaps test set. </p>

        <!-- Performance -->
        <p class="auto-style5">&nbsp;</p>
        <p id="performance" , class="auto-style4", style="color:blue;"><strong><u>Generated audio Samples by proposed dual-conditioned diffusion model.</u></strong></p>

        
        <!-- Audio 01 -----------------------------------------------------------------------------------------------------------------------> 
        <center>
          <table width="1500" border="1">
            <td width="300">
              <p><center><img src="audios/mix_3spk___12-37.png" width="300" height="150" /></center></p>
              <p><center><audio width="490" controls><source src="audios/mix_3spk___12-37.wav" type="audio/wav" >Your browser does not support the audio element.</audio></center></p>
              <p><center><strong>No Processing (Mixture = S1 + S2 + S4 + echo + noise)</strong></center></p>
              <p><center><strong>Speakers speaking in Zone1, Zone2, and Zone4 (The speaker in Zone3 is absent in this example. The mixture contains overlapping speech, echo, and noise.)</strong></center></p>
            </td>
          </table>
        </center>
         <p id="performance" , class="auto-style4", style="color:blue;"><strong><u>5 Real-world recording demo 3: Separated signals of four car zones by the proposed system (xv).</u></strong></p>
        
        <!-- Audio 03 : Real demo3 -----------------------------------------------------------------------------------------------------------------------> 
        <center>
          <table width="1500" border="1">
            <td width="300">
              <p><center><img src="audios_real_demo3/mix_1st-48-p00000030-00000517-f-norm-49-b.png" width="300" height="150" /></center></p>
              <p><center><audio width="490" controls><source src="audios_real_demo3/mix_1st-48-p00000030-00000517-f-norm-49-b.wav" type="audio/wav" >Your browser does not support the audio element.</audio></center></p>
              <p><center><strong>No Processing (Mixture = S3 + S4 + noise)</strong></center></p>
              <p><center><strong>Speakers speaking in Zone3 and Zone4 (The speakers in Zone1 and Zone2 are absent in this example. Challenging case: as Zone3 and Zone4 are both in the back-seats as shown in Fig. 1.)</strong></center></p>
            </td>
          </table>
        </center>
         
         



        

</body>

</html>
